<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous" defer></script><script class="next-config" data-name="main" type="application/json">{"hostname":"mjy.js.org","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.25.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":"ture","style":"default"},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js" defer></script><meta name="description" content="IVIF任务进展和应用概述：从数据兼容性到任务适配性"><meta property="og:type" content="article"><meta property="og:title" content="【论文阅读 | TPAMI | IVIF任务进展和应用概述：从数据兼容性到任务适配性】"><meta property="og:url" content="https://mjy.js.org/2025/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20%20TPAMI%202024%20%20IVIF%E4%BB%BB%E5%8A%A1%E8%BF%9B%E5%B1%95%E5%92%8C%E5%BA%94%E7%94%A8%E6%A6%82%E8%BF%B0%EF%BC%9A%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%85%BC%E5%AE%B9%E6%80%A7%E5%88%B0%E4%BB%BB%E5%8A%A1%E9%80%82%E9%85%8D%E6%80%A7%E3%80%91/index.html"><meta property="og:site_name" content="TranquilYu&#39;s Blog"><meta property="og:description" content="IVIF任务进展和应用概述：从数据兼容性到任务适配性"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://mjy.js.org/images/image-20251129212052527.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130080141964.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130080645753.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130080539232.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130080612639.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130081520096.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130081520096.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130081520096.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130093438617.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130100736225.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130101340887.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130101421522.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130101959966.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130102235155.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130102417905.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130102532854.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130102609933.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130102629383.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130103612720.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130103701946.png"><meta property="og:image" content="https://mjy.js.org/images/image-20251130103948338.png"><meta property="article:published_time" content="2025-11-29T00:00:00.000Z"><meta property="article:modified_time" content="2025-11-30T02:40:24.792Z"><meta property="article:author" content="MaJianyu"><meta property="article:tag" content="IVIF任务"><meta property="article:tag" content="多光谱目标检测"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://mjy.js.org/images/image-20251129212052527.png"><link rel="canonical" href="https://mjy.js.org/2025/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20%20TPAMI%202024%20%20IVIF%E4%BB%BB%E5%8A%A1%E8%BF%9B%E5%B1%95%E5%92%8C%E5%BA%94%E7%94%A8%E6%A6%82%E8%BF%B0%EF%BC%9A%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%85%BC%E5%AE%B9%E6%80%A7%E5%88%B0%E4%BB%BB%E5%8A%A1%E9%80%82%E9%85%8D%E6%80%A7%E3%80%91/"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://mjy.js.org/2025/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20%20TPAMI%202024%20%20IVIF%E4%BB%BB%E5%8A%A1%E8%BF%9B%E5%B1%95%E5%92%8C%E5%BA%94%E7%94%A8%E6%A6%82%E8%BF%B0%EF%BC%9A%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%85%BC%E5%AE%B9%E6%80%A7%E5%88%B0%E4%BB%BB%E5%8A%A1%E9%80%82%E9%85%8D%E6%80%A7%E3%80%91/","path":"2025/11/29/【论文阅读  TPAMI 2024  IVIF任务进展和应用概述：从数据兼容性到任务适配性】/","title":"【论文阅读 | TPAMI | IVIF任务进展和应用概述：从数据兼容性到任务适配性】"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>【论文阅读 | TPAMI | IVIF任务进展和应用概述：从数据兼容性到任务适配性】 | TranquilYu's Blog</title><script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous" defer></script><script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script><script src="/js/pjax.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.5.0/search.js" integrity="sha256-xFC6PJ82SL9b3WkGjFavNiA9gm5z6UBxWPiu4CYjptg=" crossorigin="anonymous" defer></script><script src="/js/third-party/search/local-search.js" defer></script><script src="/js/third-party/fancybox.js" defer></script><script src="/js/third-party/pace.js" defer></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":"flase","tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script><script src="/js/third-party/math/mathjax.js" defer></script><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g==" crossorigin="anonymous" referrerpolicy="no-referrer"><link rel="alternate" href="/atom.xml" title="TranquilYu's Blog" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><div class="column"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">TranquilYu's Blog</p><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger" aria-label="搜索" role="button"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E5%BC%95%E8%A8%80%EF%BC%9A%E4%BB%8E%E5%85%89%E8%B0%B1%E5%88%B0-IVIF"><span class="nav-number">1.</span> <span class="nav-text">1. 引言：从光谱到 IVIF</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E5%8F%AF%E8%A7%81%E5%85%89%EF%BC%88VIS%EF%BC%89%E4%B8%8E%E7%BA%A2%E5%A4%96%EF%BC%88IR%EF%BC%89%E5%9B%9E%E9%A1%BE"><span class="nav-number">1.0.1.</span> <span class="nav-text">1.1 可见光（VIS）与红外（IR）回顾</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-IVIF-%E7%9A%84%E4%BB%BB%E5%8A%A1%E5%AE%9A%E4%BD%8D"><span class="nav-number">1.0.2.</span> <span class="nav-text">1.2 IVIF 的任务定位</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E8%BF%99%E7%AF%87-TPAMI-%E7%BB%BC%E8%BF%B0%E5%9C%A8%E5%81%9A%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">1.0.3.</span> <span class="nav-text">1.3 这篇 TPAMI 综述在做什么？</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E4%BB%BB%E5%8A%A1%E5%AE%9A%E4%B9%89%E4%B8%8E%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="nav-number">2.</span> <span class="nav-text">2. 任务定义与整体流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-IVIF-%E4%BB%BB%E5%8A%A1%E5%AE%9A%E4%B9%89"><span class="nav-number">2.0.1.</span> <span class="nav-text">2.1 IVIF 任务定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E4%B8%AD%E7%9A%84%E4%B8%A4%E5%A4%A7%E6%A0%B8%E5%BF%83%E9%9A%BE%E7%82%B9"><span class="nav-number">2.0.2.</span> <span class="nav-text">2.2 实际应用中的两大核心难点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%88%B0%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B"><span class="nav-number">2.0.3.</span> <span class="nav-text">2.3 从数据到任务的完整流程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0%EF%BC%9A%E6%96%B9%E6%B3%95%E3%80%81%E4%BB%BB%E5%8A%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">3.</span> <span class="nav-text">3. 文献综述：方法、任务与数据兼容性</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E7%94%A8%E4%BA%8E%E8%A7%86%E8%A7%89%E5%A2%9E%E5%BC%BA%E7%9A%84%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 用于视觉增强的融合方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-%E5%9F%BA%E4%BA%8E%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88Auto-Encoder%EF%BC%89%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1 基于自编码器（Auto Encoder）的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%88a%EF%BC%89%E8%9E%8D%E5%90%88%E8%A7%84%E5%88%99%E6%94%B9%E8%BF%9B"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">（a）融合规则改进</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%88b%EF%BC%89%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%88%9B%E6%96%B0"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">（b）网络架构创新</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-%E5%9F%BA%E4%BA%8E-CNN-%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2 基于 CNN 的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E4%BC%98%E5%8C%96%E9%A9%B1%E5%8A%A8-CNN%EF%BC%88Optimization-inspired%EF%BC%89"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">（1）优化驱动 CNN（Optimization-inspired）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E9%A9%B1%E5%8A%A8%EF%BC%88Loss-is-almost-everything%EF%BC%89"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">（2）损失函数驱动（Loss is almost everything）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%883%EF%BC%89%E6%9E%B6%E6%9E%84%E5%88%9B%E6%96%B0%EF%BC%88Architecture-matters%EF%BC%89"><span class="nav-number">3.1.2.3.</span> <span class="nav-text">（3）架构创新（Architecture matters）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-%E5%9F%BA%E4%BA%8E-GAN-%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.3.</span> <span class="nav-text">3.1.3 基于 GAN 的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E5%8D%95%E5%88%A4%E5%88%AB%E5%99%A8%EF%BC%88Single-Discriminator%EF%BC%89"><span class="nav-number">3.1.3.1.</span> <span class="nav-text">（1）单判别器（Single Discriminator）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E5%8F%8C%E5%88%A4%E5%88%AB%E5%99%A8%EF%BC%88Dual-Discriminators%EF%BC%89"><span class="nav-number">3.1.3.2.</span> <span class="nav-text">（2）双判别器（Dual Discriminators）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-4-%E5%9F%BA%E4%BA%8E-Transformer-%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.4.</span> <span class="nav-text">3.1.4 基于 Transformer 的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E8%A1%A8%E6%96%B9%E6%B3%95"><span class="nav-number">3.1.4.1.</span> <span class="nav-text">代表方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9%E5%B0%8F%E7%BB%93"><span class="nav-number">3.1.4.2.</span> <span class="nav-text">优缺点小结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E9%9D%A2%E5%90%91%E5%BA%94%E7%94%A8%E7%9A%84%E8%9E%8D%E5%90%88%EF%BC%88%E4%BB%BB%E5%8A%A1%E9%80%82%E9%85%8D%EF%BC%89"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 面向应用的融合（任务适配）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-%E9%9D%A2%E5%90%91%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E8%9E%8D%E5%90%88"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.1 面向目标检测的融合</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89TarDAL%EF%BC%9A%E5%8F%8C%E5%B1%82%E4%BC%98%E5%8C%96%E7%9A%84%E7%9B%AE%E6%A0%87%E6%84%9F%E7%9F%A5%E8%9E%8D%E5%90%88"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">（1）TarDAL：双层优化的目标感知融合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89DetFusion%EF%BC%9A%E6%A3%80%E6%B5%8B%E9%A9%B1%E5%8A%A8%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8C%87%E5%AF%BC%E8%9E%8D%E5%90%88"><span class="nav-number">3.2.1.2.</span> <span class="nav-text">（2）DetFusion：检测驱动注意力指导融合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%883%EF%BC%89MoE-Fusion%EF%BC%9A%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%EF%BC%88Mixture-of-Experts%EF%BC%89"><span class="nav-number">3.2.1.3.</span> <span class="nav-text">（3）MoE-Fusion：混合专家（Mixture of Experts）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%884%EF%BC%89MetaFusion%EF%BC%9A%E5%85%83%E7%89%B9%E5%BE%81%E5%B5%8C%E5%85%A5"><span class="nav-number">3.2.1.4.</span> <span class="nav-text">（4）MetaFusion：元特征嵌入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%885%EF%BC%89%E4%B8%8D%E6%98%BE%E5%BC%8F%E7%94%9F%E6%88%90%E8%9E%8D%E5%90%88%E5%9B%BE%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95%EF%BC%88%E5%AF%B9%E7%85%A7%EF%BC%89"><span class="nav-number">3.2.1.5.</span> <span class="nav-text">（5）不显式生成融合图的多模态检测方法（对照）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-%E9%9D%A2%E5%90%91%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9A%84%E8%9E%8D%E5%90%88"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.2 面向语义分割的融合</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89SeAFusion%EF%BC%9A%E8%9E%8D%E5%90%88-%E5%88%86%E5%89%B2%E7%BA%A7%E8%81%94-%E8%AF%AD%E4%B9%89%E6%8D%9F%E5%A4%B1"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">（1）SeAFusion：融合 + 分割级联 + 语义损失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89SuperFusion%EF%BC%9A%E5%A4%9A%E5%8A%9F%E8%83%BD%E9%85%8D%E5%87%86-%E8%9E%8D%E5%90%88-%E8%AF%AD%E4%B9%89%E6%84%9F%E7%9F%A5%E6%A1%86%E6%9E%B6"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">（2）SuperFusion：多功能配准 + 融合 + 语义感知框架</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%883%EF%BC%89PSFusion%EF%BC%9A%E6%B8%90%E8%BF%9B%E5%BC%8F%E8%AF%AD%E4%B9%89%E6%B3%A8%E5%85%A5"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">（3）PSFusion：渐进式语义注入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%884%EF%BC%89SegMiF%EF%BC%9A%E5%8F%8C%E4%BB%BB%E5%8A%A1%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E8%81%94%E5%90%88%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.2.2.4.</span> <span class="nav-text">（4）SegMiF：双任务相关性的联合学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%885%EF%BC%89MRFS-CAF-TIMFusion-%E7%AD%89%E7%BB%9F%E4%B8%80%E6%84%9F%E7%9F%A5%E9%A9%B1%E5%8A%A8%E6%A1%86%E6%9E%B6"><span class="nav-number">3.2.2.5.</span> <span class="nav-text">（5）MRFS &#x2F; CAF &#x2F; TIMFusion 等统一感知驱动框架</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-%E5%85%B6%E4%BB%96%E6%84%9F%E7%9F%A5%E4%BB%BB%E5%8A%A1%EF%BC%88%E7%95%A5%EF%BC%89"><span class="nav-number">3.2.3.</span> <span class="nav-text">3.2.3 其他感知任务（略）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-%E6%95%B0%E6%8D%AE%E5%85%BC%E5%AE%B9%E6%80%A7%EF%BC%9A%E6%9C%AA%E9%85%8D%E5%87%86%E3%80%81%E9%80%9A%E7%94%A8%E8%9E%8D%E5%90%88%E4%B8%8E%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 数据兼容性：未配准、通用融合与对抗攻击</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-%E6%9C%AA%E9%85%8D%E5%87%86%EF%BC%88Misaligned%EF%BC%89%E5%9B%BE%E5%83%8F%E7%9A%84%E8%9E%8D%E5%90%88"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.3.1 未配准（Misaligned）图像的融合</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E5%9F%BA%E4%BA%8E%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E7%9A%84%E9%85%8D%E5%87%86-%E8%9E%8D%E5%90%88%E8%8C%83%E5%BC%8F"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">（1）基于风格迁移的配准 + 融合范式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E5%9F%BA%E4%BA%8E%E6%BD%9C%E5%9C%A8%E7%A9%BA%E9%97%B4%E7%9A%84%E6%A8%A1%E6%80%81%E6%97%A0%E5%85%B3%E7%89%B9%E5%BE%81"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">（2）基于潜在空间的模态无关特征</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-%E9%80%9A%E7%94%A8%E8%9E%8D%E5%90%88%E6%A1%86%E6%9E%B6%EF%BC%88General-Fusion%EF%BC%89"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.3.2 通用融合框架（General Fusion）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%88%9B%E6%96%B0%E6%96%B9%E5%90%91"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">（1）损失函数创新方向</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E6%96%B9%E5%90%91"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">（2）架构设计方向</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-3-%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B8%8E%E9%B2%81%E6%A3%92%E6%80%A7"><span class="nav-number">3.3.3.</span> <span class="nav-text">3.3.3 对抗攻击与鲁棒性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-%E6%9E%B6%E6%9E%84%E6%80%BB%E7%BB%93%E4%B8%8E%E8%AE%A8%E8%AE%BA"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 架构总结与讨论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93%E4%B8%8E%E8%AE%A8%E8%AE%BA"><span class="nav-number">3.5.</span> <span class="nav-text">3.5 损失函数总结与讨论</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">4.</span> <span class="nav-text">4. 基准数据集与评估指标</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A6%82%E8%A7%88"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 数据集概览</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-%E6%97%A9%E6%9C%9F%E9%9D%A2%E5%90%91%E8%9E%8D%E5%90%88%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">4.1.1.</span> <span class="nav-text">4.1.1 早期面向融合的数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">4.1.2.</span> <span class="nav-text">4.1.2 目标检测数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-3-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">4.1.3.</span> <span class="nav-text">4.1.3 语义分割数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 评估指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-%E8%9E%8D%E5%90%88%E8%B4%A8%E9%87%8F%E6%8C%87%E6%A0%87%EF%BC%889-%E4%B8%AA%EF%BC%89"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.2.1 融合质量指标（9 个）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-2-%E9%85%8D%E5%87%86%E6%8C%87%E6%A0%87"><span class="nav-number">4.2.2.</span> <span class="nav-text">4.2.2 配准指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-3-%E6%84%9F%E7%9F%A5%E4%BB%BB%E5%8A%A1%E6%8C%87%E6%A0%87"><span class="nav-number">4.2.3.</span> <span class="nav-text">4.2.3 感知任务指标</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-%E6%80%A7%E8%83%BD%E6%80%BB%E7%BB%93%E4%B8%8E%E6%B6%88%E8%9E%8D%E5%AF%B9%E6%AF%94"><span class="nav-number">5.</span> <span class="nav-text">5. 性能总结与消融对比</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E8%B4%A8%E9%87%8F%EF%BC%88TNO-RoadScene-M%C2%B3FD%EF%BC%89"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 图像融合质量（TNO &#x2F; RoadScene &#x2F; M³FD）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-%E6%9C%AA%E5%AF%B9%E9%BD%90%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88%E6%80%A7%E8%83%BD%EF%BC%88RoadScene-M%C2%B3FD%EF%BC%89"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 未对齐图像融合性能（RoadScene &#x2F; M³FD）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-%E9%9D%A2%E5%90%91%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E8%9E%8D%E5%90%88%E6%80%A7%E8%83%BD%EF%BC%88M%C2%B3FD-YOLOv5%EF%BC%89"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 面向目标检测的融合性能（M³FD + YOLOv5）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-%E9%9D%A2%E5%90%91%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9A%84%E8%9E%8D%E5%90%88%E6%80%A7%E8%83%BD%EF%BC%88FMB-SegFormer%EF%BC%89"><span class="nav-number">5.4.</span> <span class="nav-text">5.4 面向语义分割的融合性能（FMB + SegFormer）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5-%E8%AE%A1%E7%AE%97%E6%95%88%E7%8E%87%E4%B8%8E%E8%B5%84%E6%BA%90%E6%B6%88%E8%80%97"><span class="nav-number">5.5.</span> <span class="nav-text">5.5 计算效率与资源消耗</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-%E6%8C%91%E6%88%98%E4%B8%8E%E6%9C%AA%E6%9D%A5%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91"><span class="nav-number">6.</span> <span class="nav-text">6. 挑战与未来研究方向</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-%E6%9C%AA%E9%85%8D%E5%87%86-%E5%AF%B9%E6%8A%97%E9%B2%81%E6%A3%92%E6%80%A7"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 未配准 &amp; 对抗鲁棒性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-%E4%BB%BB%E5%8A%A1%E9%A9%B1%E5%8A%A8%E7%9A%84%E8%9E%8D%E5%90%88%E8%AE%BE%E8%AE%A1"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 任务驱动的融合设计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-%E6%9B%B4%E5%A4%A7%E3%80%81%E6%9B%B4%E7%9C%9F%E5%AE%9E%E3%80%81%E6%9B%B4%E9%9A%BE%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 更大、更真实、更难的数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-4-%E6%9B%B4%E5%90%88%E7%90%86%E7%9A%84%E8%AF%84%E4%BB%B7%E4%BD%93%E7%B3%BB"><span class="nav-number">6.4.</span> <span class="nav-text">6.4 更合理的评价体系</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-5-%E8%BD%BB%E9%87%8F%E5%8C%96%E4%B8%8E%E9%83%A8%E7%BD%B2%E5%8F%8B%E5%A5%BD"><span class="nav-number">6.5.</span> <span class="nav-text">6.5 轻量化与部署友好</span></a></li></ol></li></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">MaJianyu</p><div class="site-description" itemprop="description">永远相信，美好的事情即将发生。</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">22</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">42</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/majianyu2007" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;majianyu2007" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://space.bilibili.com/600064036" title="BiliBili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;600064036" rel="noopener me" target="_blank"><i class="fa-brands fa-bilibili fa-fw"></i>BiliBili</a></span></div><div class="cc-license animated" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a></div></div></div></div></aside></div><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://mjy.js.org/2025/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20%20TPAMI%202024%20%20IVIF%E4%BB%BB%E5%8A%A1%E8%BF%9B%E5%B1%95%E5%92%8C%E5%BA%94%E7%94%A8%E6%A6%82%E8%BF%B0%EF%BC%9A%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%85%BC%E5%AE%B9%E6%80%A7%E5%88%B0%E4%BB%BB%E5%8A%A1%E9%80%82%E9%85%8D%E6%80%A7%E3%80%91/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="MaJianyu"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="TranquilYu's Blog"><meta itemprop="description" content="永远相信，美好的事情即将发生。"></span><span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="【论文阅读 | TPAMI | IVIF任务进展和应用概述：从数据兼容性到任务适配性】 | TranquilYu's Blog"><meta itemprop="description" content="IVIF任务进展和应用概述：从数据兼容性到任务适配性"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">【论文阅读 | TPAMI | IVIF任务进展和应用概述：从数据兼容性到任务适配性】</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2025-11-29 08:00:00" itemprop="dateCreated datePublished" datetime="2025-11-29T08:00:00+08:00">2025-11-29</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-11-30 10:40:24" itemprop="dateModified" datetime="2025-11-30T10:40:24+08:00">2025-11-30</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a> </span></span><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>18k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>32 分钟</span></span></div><div class="post-description">IVIF任务进展和应用概述：从数据兼容性到任务适配性</div></div></header><div class="post-body" itemprop="articleBody"><p>[TOC]</p><p><img data-src="/./../images/image-20251129212052527.png" alt="image-20251129212052527"></p><p>题目：Infrared and Visible Image Fusion: From Data Compatibility to Task Adaption</p><p>会议：IEEE Transactions on Pattern Analysis and Machine Intelligence （IEEE TPAMI）</p><p>代码：<a target="_blank" rel="noopener" href="https://github.com/RollingPlain/IVIF_ZOO">https://github.com/RollingPlain/IVIF_ZOO</a></p><p>时间：23 December 2024</p><p><strong>本文将近年来的IVIF方法进行了总结分析，可以将本文当做快速学习IVIF方法的途径</strong></p><hr><h1 id="1-引言：从光谱到-IVIF"><a href="#1-引言：从光谱到-IVIF" class="headerlink" title="1. 引言：从光谱到 IVIF"></a>1. 引言：从光谱到 IVIF</h1><h3 id="1-1-可见光（VIS）与红外（IR）回顾"><a href="#1-1-可见光（VIS）与红外（IR）回顾" class="headerlink" title="1.1 可见光（VIS）与红外（IR）回顾"></a>1.1 可见光（VIS）与红外（IR）回顾</h3><ul><li><p><strong>可见光 VIS</strong></p><ul><li>传感器记录可见波段的反射光，也就是我们常见的 RGB 图像</li><li>优点：纹理、颜色、结构信息丰富，细节清晰</li><li>缺点：高度依赖环境光，在夜晚、逆光、雨雾、烟雾环境下质量急剧下降</li></ul></li><li><p><strong>红外 IR</strong></p><ul><li>记录物体表面的热辐射，不依赖可见光</li><li>优点：弱光 &#x2F; 全黑环境也能看见“热目标”（行人、车辆等）</li><li>缺点：纹理少、对比度低，看起来“灰、糊”，不适合直接做人眼视觉展示</li></ul></li></ul><blockquote><p>典型互补性：<br>VIS：看清<strong>细节、结构</strong><br>IR：看清<strong>重要目标</strong><br>→ 于是不难想到：把两者融合成一张“既清晰又目标突出的”图像。</p></blockquote><h3 id="1-2-IVIF-的任务定位"><a href="#1-2-IVIF-的任务定位" class="headerlink" title="1.2 IVIF 的任务定位"></a>1.2 IVIF 的任务定位</h3><p>红外与可见光图像融合（IVIF）的目标：</p><blockquote><p>给定一对 IR &#x2F; VIS 图像，生成一张融合图像 F，使其包含：</p><ul><li>VIS 的纹理与结构信息</li><li>IR 的显著目标与轮廓信息<br>同时兼顾视觉效果（给人看）与感知性能（给机器看）。</li></ul></blockquote><p>IVIF 早期主要是作为<strong>图像增强技术</strong>，服务于夜视、监控等“人眼观察”；<br>近年来逐渐变成<strong>多模态感知的前端模块</strong>，服务于：</p><ul><li>目标检测（行人 &#x2F; 车辆）</li><li>语义分割（道路 &#x2F; 建筑 &#x2F; 行人）</li><li>目标跟踪、人群计数、显著目标检测、深度估计等</li></ul><h3 id="1-3-这篇-TPAMI-综述在做什么？"><a href="#1-3-这篇-TPAMI-综述在做什么？" class="headerlink" title="1.3 这篇 TPAMI 综述在做什么？"></a>1.3 这篇 TPAMI 综述在做什么？</h3><p><img data-src="/./../images/image-20251130080141964.png" alt="image-20251130080141964"></p><p>论文的核心点不是简单罗列方法，而是提出了一个<strong>三维视角</strong>：</p><ol><li><p><strong>数据兼容性（Data Compatibility）</strong></p><ul><li>未配准图像如何处理？</li><li>对抗样本 &#x2F; 传感器扰动如何应对？</li></ul></li><li><p><strong>融合方法（Image Fusion）</strong></p><ul><li>AE &#x2F; CNN &#x2F; GAN &#x2F; Transformer &#x2F; 通用融合框架</li><li>损失函数和网络架构如何演化？</li></ul></li><li><p><strong>任务适配（Task Adaption）</strong></p><ul><li>如何为检测 &#x2F; 分割等下游任务设计“任务驱动”的融合网络？</li><li>如何评估“好看”和“好用”的平衡？</li></ul></li></ol><p>本综述整理了 <strong>180+</strong> 种深度学习 IVIF 方法，结合大量对比实验、表格与可视化结果，给出了当前研究现状与未来趋势。</p><hr><h1 id="2-任务定义与整体流程"><a href="#2-任务定义与整体流程" class="headerlink" title="2. 任务定义与整体流程"></a>2. 任务定义与整体流程</h1><h3 id="2-1-IVIF-任务定义"><a href="#2-1-IVIF-任务定义" class="headerlink" title="2.1 IVIF 任务定义"></a>2.1 IVIF 任务定义</h3><p>给定一对由不同传感器采集的 IR &#x2F; VIS 图像：</p><ul><li>记为：(I_{IR}, I_{VIS})</li><li>任务：学习一个融合算子 ( \mathcal{F}(·) )，得到融合图 ( I_F &#x3D; \mathcal{F}(I_{IR}, I_{VIS}))</li></ul><p>要求：</p><ul><li>信息量不低于任一模态；</li><li>IR 中的热目标在融合图中清晰可见；</li><li>VIS 中的纹理、结构细节得到较好保留；</li><li>融合图既可供人眼理解，又利于下游任务网络（检测 &#x2F; 分割等）处理。</li></ul><h3 id="2-2-实际应用中的两大核心难点"><a href="#2-2-实际应用中的两大核心难点" class="headerlink" title="2.2 实际应用中的两大核心难点"></a>2.2 实际应用中的两大核心难点</h3><ol><li><p><strong>未必有“完美配准”的数据</strong></p><ul><li>不同相机视角、安装方式、分辨率都不一样；</li><li>精准像素级对齐很难，特别是移动平台、动态场景。</li></ul><p>也就是说，一般没有红外与可见光两张图重合起来一毛一样的情况，多多少少会有一些偏差。</p></li><li><p><strong>很多方法只优化“图像质量”，忽视“任务表现”</strong></p><ul><li><p>只看 MI &#x2F; VIF &#x2F; AG &#x2F; SF 等融合指标；</p></li><li><p>但在目标检测 &#x2F; 语义分割等任务上表现平平甚至退化。</p></li></ul></li></ol><p>正因如此，我们需要“<strong>任务驱动的融合</strong>”：在训练时就把 YOLO &#x2F; SegFormer 等网络拉进来一起优化。</p><h3 id="2-3-从数据到任务的完整流程"><a href="#2-3-从数据到任务的完整流程" class="headerlink" title="2.3 从数据到任务的完整流程"></a>2.3 从数据到任务的完整流程</h3><p><img data-src="/./../images/image-20251130080645753.png" alt="image-20251130080645753"></p><p>可以将上图抽象成三步：</p><blockquote><p><strong>数据兼容性 → 融合网络 → 任务网络</strong></p></blockquote><ul><li><strong>Data</strong>：IR&#x2F;VIS 图像可能未对齐、含噪声、甚至遭到对抗攻击，需要配准 &#x2F; 校正 &#x2F; 防御（这里比如随机在红外或可见光图像上挖去一块，模型可能会错误地放大这部分错误，因此要进行防御）；</li><li><strong>Fusion</strong>：把数据映射到融合图像或多模态特征空间；</li><li><strong>Task</strong>：检测、分割、SOD、计数、深度估计等感知任务。</li></ul><hr><h1 id="3-文献综述：方法、任务与数据兼容性"><a href="#3-文献综述：方法、任务与数据兼容性" class="headerlink" title="3. 文献综述：方法、任务与数据兼容性"></a>3. 文献综述：方法、任务与数据兼容性</h1><p>本节是组会的主要部分，按三条线讲：</p><ol><li>用于<strong>视觉增强</strong>的融合方法（AE&#x2F;CNN&#x2F;GAN&#x2F;Transformer）；</li><li>面向<strong>应用任务</strong>的融合（检测 &#x2F; 分割 &#x2F; 其他）；</li><li>针对<strong>数据兼容性</strong>的未配准、通用融合框架与对抗攻击。</li></ol><p><img data-src="/./../images/image-20251130080539232.png" alt="image-20251130080539232"></p><blockquote><p>图4. 包含典型融合方法的分类桑基图。</p></blockquote><p><img data-src="/./../images/image-20251130080612639.png" alt="image-20251130080612639"></p><hr><h2 id="3-1-用于视觉增强的融合方法"><a href="#3-1-用于视觉增强的融合方法" class="headerlink" title="3.1 用于视觉增强的融合方法"></a>3.1 用于视觉增强的融合方法</h2><blockquote><p>假设数据已经对齐，目标是“图像本身要好看、信息量大”。</p></blockquote><p>这类方法大多遵循统一 pipeline）：</p><blockquote><p>特征提取（Encoder &#x2F; Backbone）</p><p>特征融合（Fusion Rule &#x2F; Attention）</p><p>图像重建（Decoder &#x2F; Generator）</p></blockquote><p><img data-src="/./../images/image-20251130081520096.png" alt="image-20251130081520096"></p><h3 id="3-1-1-基于自编码器（Auto-Encoder）的方法"><a href="#3-1-1-基于自编码器（Auto-Encoder）的方法" class="headerlink" title="3.1.1 基于自编码器（Auto Encoder）的方法"></a>3.1.1 基于自编码器（Auto Encoder）的方法</h3><p><strong>基本范式</strong></p><ol><li><p>预训练自编码器 AE：</p><ul><li>可以用 VIS 图（单模态）或 IR&#x2F;VIS 图对（多模态）进行训练；</li><li>AE 学到“如何从特征重建图像”。</li></ul></li><li><p>图像重建（推理阶段）：</p><ul><li>IR &#x2F; VIS 分别通过共享的 Encoder 得到特征 (F_{IR}, F_{VIS})；</li><li>按<strong>某种融合规则</strong>得到 (F_{Fuse})；</li><li>Decoder 将 (F_{Fuse}) 重建为融合图。</li></ul></li></ol><p><strong>核心问题</strong>：哪种“融合规则”。</p><h4 id="（a）融合规则改进"><a href="#（a）融合规则改进" class="headerlink" title="（a）融合规则改进"></a>（a）融合规则改进</h4><ul><li><p><strong>DenseFuse</strong></p><ul><li>Encoder：多层稠密卷积，提取多尺度特征；</li><li>Decoder：对称结构重建；</li><li>提出两类经典融合策略：<ol><li><strong>Add</strong>：(F &#x3D; F_{IR} + F_{VIS})</li><li><strong>L1-based Softmax</strong>：按特征 L1 范数计算权重，显著响应权重更大；</li></ol></li><li>用像素重建损失（L2）+ SSIM 保证结果接近源图。</li></ul></li><li><p><strong>MFEIF</strong></p><ul><li>多尺度特征提取 + 边缘引导注意力；</li><li>在融合时给边缘位置更高权重，让细节更清晰。</li></ul></li><li><p><strong>SFAFuse &#x2F; SEDRFuse 等</strong></p><ul><li>自监督策略 + 注意力加权；</li><li>用更多“显著性 &#x2F; 结构信息”引导融合规则，而非简单加权或 max。</li></ul></li></ul><h4 id="（b）网络架构创新"><a href="#（b）网络架构创新" class="headerlink" title="（b）网络架构创新"></a>（b）网络架构创新</h4><ul><li><p><strong>RFN-Nest</strong></p><ul><li>引入残差融合网络（RFN），把“融合规则”本身做成一个小网络；</li><li>整体结构类似 U-Net++ 的嵌套连接，更利于多尺度信息流动；</li><li>关注在“保护结构不破坏 + 增强细节”。</li></ul></li><li><p><strong>Re2Fusion</strong></p><ul><li>密集残差 + 双非局部（Non-local）模块；</li><li>一个 Non-local 看“同模态内部长程关系”，另一个看“跨模态互补关系”。</li></ul></li><li><p><strong>SMoA</strong></p><ul><li>基于 NAS 自动搜索编码器结构；</li><li>学到“适合 IVIF 的特征层次分布”，配合显著性权重作为融合规则。</li></ul></li></ul><p>现有的 AE 方法可分为两类：</p><ul><li><p>融合规则和数据集成方面的增强，旨在改进多模态特征合成。</p></li><li><p>网络架构方面的创新，包括引入新层和修改连接。</p></li></ul><hr><h3 id="3-1-2-基于-CNN-的方法"><a href="#3-1-2-基于-CNN-的方法" class="headerlink" title="3.1.2 基于 CNN 的方法"></a>3.1.2 基于 CNN 的方法</h3><p><img data-src="/./../images/image-20251130081520096.png" alt="image-20251130081520096"></p><blockquote><p>当前数量最多的一类。此类算法的关键优势在于它们能够从数据中自主学习复杂的高级特征。</p></blockquote><p>仍然遵循三步：<strong>特征提取 → 融合 → 重建</strong>，区别在于：</p><ol><li>是否由优化模型展开（Optimization-inspired）；</li><li>损失函数如何设计（Loss-driven）；</li><li>网络结构如何堆叠（Architecture）。</li></ol><h4 id="（1）优化驱动-CNN（Optimization-inspired）"><a href="#（1）优化驱动-CNN（Optimization-inspired）" class="headerlink" title="（1）优化驱动 CNN（Optimization-inspired）"></a>（1）优化驱动 CNN（Optimization-inspired）</h4><p>思想：把传统的优化模型或变分模型“展开”为网络，每一层对应一次迭代。</p><ul><li><p><strong>LRRNet（低秩表示 + TPAMI）</strong></p><ul><li>假设图像可以分解为：<strong>低秩结构 + 稀疏细节</strong>；</li><li>原公式需要解一个带低秩约束的优化问题；</li><li>作者把迭代求解步骤写成可学习的网络模块（unrolling）：<ul><li>每一层更新低秩分量 &#x2F; 稀疏分量 &#x2F; 拉格朗日乘子；</li></ul></li><li>优点：结构有清晰物理含义，可解释性强。</li></ul></li><li><p><strong>BIMDL（双层优化）</strong></p><ul><li>外层：控制“基础层 &#x2F; 細节层”如何分解；</li><li>内层：在给定分解的情况下，优化融合图的重建误差；</li><li>通过双层目标，分别约束结构与纹理表示。</li></ul></li><li><p><strong>AUIF（算法展开）</strong></p><ul><li>将变分模型中的低频 &#x2F; 高频项分成两个分支；</li><li>使用卷积模块模拟原本的梯度下降 &#x2F; 迭代规约过程；</li><li>实际效果：既平滑噪声，又保留清晰边缘。</li></ul></li></ul><blockquote><p>优化驱动 CNN 的共同特点：<br>“每一层做一小步优化”，有可解释性，而且适合写理论&#x2F;推导。</p></blockquote><h4 id="（2）损失函数驱动（Loss-is-almost-everything）"><a href="#（2）损失函数驱动（Loss-is-almost-everything）" class="headerlink" title="（2）损失函数驱动（Loss is almost everything）"></a>（2）损失函数驱动（Loss is almost everything）</h4><p>无监督 IVIF 没有 GT 融合图 → 损失函数几乎决定模型上限。</p><ul><li><p><strong>PIAFusion（光照感知损失）</strong></p><ul><li>引入子网络预测场景光照图 (L(x))；</li><li>在暗区更信 IR、在亮区更信 VIS；</li><li>在总损失中加入光照加权项，使模型学习到“不同区域依赖不同模态”。</li></ul></li><li><p><strong>STDFusionNet（显著性引导损失）</strong></p><ul><li>先用显著性检测网络得到 IR 中“热目标掩模”；</li><li>在 Loss 中对这些区域加大权重：<ul><li>目标区域：更强调 IR → 保证目标突出；</li><li>非目标区域：平衡 IR&#x2F;VIS → 保证背景自然。</li></ul></li></ul></li><li><p><strong>PSFusion（Progressive Semantic Injection）</strong></p><ul><li>用语义分割网络抽取 semantic feature；</li><li>通过特征层渐进注入 + 语义一致性损失，使融合图保留更多语义边界（路 &#x2F; 车 &#x2F; 人）。</li></ul></li><li><p><strong>PromptF &#x2F; FILM（语义 &#x2F; 感知损失）</strong></p><ul><li>利用视觉-语言模型或大语言模型生成文本提示（prompts）；</li><li>将这些“语义提示”编码成向量，引入感知损失，引导网络在特定语义区域加强细节；</li><li>兼顾视觉效果与识别性能。</li></ul></li></ul><blockquote><p>实际上，很多方法网络”长得“差不多，真正区别在损失函数怎么写。</p></blockquote><h4 id="（3）架构创新（Architecture-matters）"><a href="#（3）架构创新（Architecture-matters）" class="headerlink" title="（3）架构创新（Architecture matters）"></a>（3）架构创新（Architecture matters）</h4><ul><li><p><strong>IGNet（CNN + GNN）</strong></p><ul><li>CNN 提取多尺度特征 → 构建图结构（像素块作为节点）；</li><li>通过图神经网络做跨模态信息传播；</li><li>适合建模远距离关联和跨模态一致性。</li></ul></li><li><p><strong>Dif-Fusion &#x2F; DDFM（扩散模型）</strong></p><ul><li>构建“融合图的扩散过程”：从噪声逐步还原；</li><li>通过正向加噪 + 逆向去噪学习图像分布；</li><li>优点：视觉质量高，细节自然；</li><li>缺点：计算极其昂贵，不适合实时部署。</li></ul></li><li><p><strong>SMoA &#x2F; TIMFusion &#x2F; 隐式架构搜索（IAS）</strong></p><ul><li>SMoA：构建“超网络”，用 NAS 自动搜索子结构（编码器 &#x2F; 融合模块）；</li><li>TIMFusion：在搜索目标中直接融合“视觉性能 + 硬件延迟”，找到既准又快的结构；</li><li>体现出“<strong>自动化结构设计 + 硬件感知</strong>”的新趋势。</li></ul></li></ul><p>在基于 CNN 的图像融合方法领域，有三种创新方法脱颖而出：</p><ul><li><p>受优化启发的 CNN 方法利用迭代集成和可学习模块来提高融合效率。</p></li><li><p>修改损失函数对 IVIF 任务中的无监督学习结果至关重要。</p></li><li><p>架构改进侧重于优化网络设计，神经架构搜索（NAS）是一种用于结构优化的专门方法。</p></li></ul><hr><h3 id="3-1-3-基于-GAN-的方法"><a href="#3-1-3-基于-GAN-的方法" class="headerlink" title="3.1.3 基于 GAN 的方法"></a>3.1.3 基于 GAN 的方法</h3><p><img data-src="/./../images/image-20251130081520096.png" alt="image-20251130081520096"></p><p>GAN 天然适合“无监督分布建模”，与 IVIF 相当契合。GAN在无需标签监督的情况下对数据分布进行建模方面已证明其有效性。这种无监督方法自然适用于 IVIF 任务，GAN 已成为主要的方法之一。：</p><ul><li>生成器 G：输入 IR&#x2F;VIS，输出融合图；</li><li>判别器 D：判断融合图像分布是否接近某模态或多模态分布。</li></ul><p>但也存在典型问题：</p><ul><li>训练不稳定；</li><li>易出现伪影；</li><li>模态不平衡（只像 VIS 或只像 IR）。</li></ul><h4 id="（1）单判别器（Single-Discriminator）"><a href="#（1）单判别器（Single-Discriminator）" class="headerlink" title="（1）单判别器（Single Discriminator）"></a>（1）单判别器（Single Discriminator）</h4><blockquote><p>单判别器方法利用原始 GAN ，将融合图像约束为与一种模态相似。</p></blockquote><ul><li><p><strong>FusionGAN</strong></p><ul><li>生成器包含两条路径：<ul><li>对比路径：保 IR 强度分布；</li><li>梯度路径：保 VIS 纹理细节；</li></ul></li><li>判别器只判断“像不像 VIS”；</li><li>问题：往往导致融合结果偏向可见光，弱化热目标。</li></ul></li><li><p><strong>GANMcC</strong></p><ul><li>将判别器改为多分类：区分 IR &#x2F; VIS &#x2F; Fused 等类别；</li><li>希望通过多分类方式减轻模态不平衡；</li><li>本质仍为单判别器，无法对不同模态施加独立约束。</li></ul></li></ul><blockquote><p>单判别器的普遍问题：监督信号偏向单模态，难以同时保留 IR + VIS 两端的优势。</p></blockquote><h4 id="（2）双判别器（Dual-Discriminators）"><a href="#（2）双判别器（Dual-Discriminators）" class="headerlink" title="（2）双判别器（Dual Discriminators）"></a>（2）双判别器（Dual Discriminators）</h4><blockquote><p>双判别器方法利用两个判别器来平衡典型的模态信息。···································································</p></blockquote><ul><li><p><strong>DDcGAN</strong></p><ul><li>生成器 G 输出 F；</li><li>两个判别器：<ul><li>(D_{VIS})：判断 F 是否“像 VIS” → 对纹理敏感；</li><li>(D_{IR})：判断 F 是否“像 IR” → 对热目标和强度敏感；</li></ul></li><li>在对抗训练中逼迫 G 在两种模态间折中，兼顾纹理与目标。</li></ul></li><li><p><strong>ICAFusion</strong></p><ul><li>在生成器中加入交互注意力（Interactive Cross Attention）模块；</li><li>强调跨模态信息交互，而非简单拼接或加权；</li><li>依然是 G + 2D 的框架结构。</li></ul></li><li><p><strong>FreqGAN</strong></p><ul><li>采用小波分解，将图像拆成不同频带；</li><li>判别器在频域中约束各频带特征，使高频细节和低频结构分别得到合理保留。</li></ul></li></ul><blockquote><p>总结：GAN 系方法在视觉效果上很亮眼，但<br>在检测 &#x2F; 分割等任务上的提升并<strong>不必然</strong>优于 CNN &#x2F; Transformer 系。</p></blockquote><hr><h3 id="3-1-4-基于-Transformer-的方法"><a href="#3-1-4-基于-Transformer-的方法" class="headerlink" title="3.1.4 基于 Transformer 的方法"></a>3.1.4 基于 Transformer 的方法</h3><blockquote><p>核心：用自注意力建模<strong>长程依赖 + 跨模态交互</strong>。</p></blockquote><p>总体上的趋势还是：<strong>CNN + Transformer 混合结构</strong>：</p><ul><li>CNN：提局部细节（边缘、纹理）；</li><li>Transformer：提全局上下文和跨模态关系。</li></ul><h4 id="代表方法"><a href="#代表方法" class="headerlink" title="代表方法"></a>代表方法</h4><ul><li><p><strong>IFT &#x2F; SwinFusion &#x2F; YDTR &#x2F; TGFuse &#x2F; DATFuse</strong></p><ul><li>典型结构：<ul><li>前几层：卷积提局部浅层特征；</li><li>后几层：Transformer 块（window &#x2F; global attention）建模长程依赖；</li></ul></li><li>SwinFusion：用 Swin Transformer 构造两种注意力模块（自注意力 + 跨模态注意力）；</li><li>YDTR：Y 型结构，一路偏局部细节，一路偏全局上下文。</li></ul></li><li><p><strong>DATFuse</strong></p><ul><li>双重注意力：同时处理空间注意力与通道注意力；</li><li>特别强调“重要区域 + 重要通道”的选择性放大。</li></ul></li><li><p><strong>CMTFusion &#x2F; CDDFuse</strong></p><ul><li>CMTFusion：跨模态 Transformer，显式去冗余，保留互补信息；</li><li>CDDFuse：双分支 CNN-Transformer 结构，两路分别分解模态相关与模态无关特征，借助 Restormer 风格模块实现无损信息传递。</li></ul></li><li><p><strong>Text-IF &#x2F; PromptF &#x2F; FILM</strong></p><ul><li>Text-IF：引入文本描述（例如“车”“行人”），做文本引导的退化感知融合；</li><li>PromptF：通过视觉-语言模型生成语义提示，指导融合网络关注特定重要区域；</li><li>FILM：利用 ChatGPT 等从图像生成语义文本，再用跨注意力把文本信息注入融合过程。</li></ul></li></ul><h4 id="优缺点小结"><a href="#优缺点小结" class="headerlink" title="优缺点小结"></a>优缺点小结</h4><ul><li><p>优点：</p><ul><li>长程依赖建模能力强；</li><li>跨模态交互更充分；</li><li>指标（MI&#x2F;VIF&#x2F;结构保持等）往往更好。</li></ul></li><li><p>缺点：</p><ul><li>模型参数多，FLOPs 高；</li><li>显存占用大，对端侧部署不友好；</li><li>需要结合蒸馏 &#x2F; 剪枝 &#x2F; NAS 等做轻量化。</li></ul></li></ul><hr><h2 id="3-2-面向应用的融合（任务适配）"><a href="#3-2-面向应用的融合（任务适配）" class="headerlink" title="3.2 面向应用的融合（任务适配）"></a>3.2 面向应用的融合（任务适配）</h2><blockquote><p>说人话，这一节就是体现“从给人看 → 给机器看”的地方。</p></blockquote><p>论文重点讲了两类任务：</p><ol><li><strong>目标检测（Detection-aware Fusion）</strong>；</li><li><strong>语义分割（Segmentation-aware Fusion）</strong>。</li></ol><p>此外还提到跟踪、人群计数、显著性检测、深度估计等任务，这些与 IVIF 关系略弱，因此本节简略带过。</p><hr><h3 id="3-2-1-面向目标检测的融合"><a href="#3-2-1-面向目标检测的融合" class="headerlink" title="3.2.1 面向目标检测的融合"></a>3.2.1 面向目标检测的融合</h3><p>背景：有了像 <strong>M³FD</strong> 这样同时包含 IR&#x2F;VIS 图像对和目标检测标注的数据集，可以直接让检测网络参与融合网络训练。</p><h4 id="（1）TarDAL：双层优化的目标感知融合"><a href="#（1）TarDAL：双层优化的目标感知融合" class="headerlink" title="（1）TarDAL：双层优化的目标感知融合"></a>（1）TarDAL：双层优化的目标感知融合</h4><blockquote><p>Liu 等人率先探索了图像融合和目标检测，提出了 <strong>TarDAL</strong> 方法，并构建了最大的多模态目标检测数据集<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>M</mi><mn>3</mn></msup><mi>F</mi><mi>D</mi></mrow><annotation encoding="application/x-tex">M^{3} FD</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:.13889em">F</span><span class="mord mathnormal" style="margin-right:.02778em">D</span></span></span></span>。TarDAL 提出了一种双层优化公式来建模两个任务之间的内在关系，并展开优化以构建目标感知的双层学习网络。</p></blockquote><ul><li><p>贡献：</p><ul><li>提出最大的多模态目标检测数据集 M³FD；</li><li>提出 TarDAL（Target-aware Dual Adversarial Learning）。</li></ul></li><li><p><strong>双层优化思想</strong>：</p><ul><li>下层（fusion-level）：给定 IR&#x2F;VIS，输出融合图；</li><li>上层（detection-level）：在融合图上训练检测网络（类似 YOLO），最大化检测 mAP；</li><li>把两个层级的优化过程展开成一个可训练网络。</li></ul></li><li><p>关键点：</p><ul><li>利用视觉显著性图（VSM）强调 IR 中的热目标；</li><li>损失函数中对目标区域加权，保证小目标（远处行人）不被淹没；</li><li>实验结果：在 M³FD 上 mAP 全面领先，尤其在小目标和退化场景（雾&#x2F;雨&#x2F;强光）上优势明显。</li></ul></li></ul><h4 id="（2）DetFusion：检测驱动注意力指导融合"><a href="#（2）DetFusion：检测驱动注意力指导融合" class="headerlink" title="（2）DetFusion：检测驱动注意力指导融合"></a>（2）DetFusion：检测驱动注意力指导融合</h4><blockquote><p>Zhao 等人引入元特征嵌入以实现目标检测和图像融合之间的兼容性。Sun 等人提出了令人瞩目的 <strong>DetFusion</strong> 方法，利用检测驱动的信息通过共享注意力机制指导融合优化。目标感知损失在从目标位置学习像素级信息方面也起着关键作用。</p></blockquote><ul><li>结构：<ul><li>融合网络 + 检测网络部分共享 backbone；</li><li>检测分支产生的 attention map 回馈给融合分支；</li></ul></li><li>思路：<ul><li>检测网络告诉融合网络“哪里是车 &#x2F; 人 &#x2F; 重要目标”；</li><li>融合网络在这些位置分配更多注意力与容量；</li></ul></li><li>效果：<ul><li>在不大幅增加计算量的前提下，提高检测性能；</li><li>在视觉上也偏向强化目标区域的对比度和边界。</li></ul></li></ul><h4 id="（3）MoE-Fusion：混合专家（Mixture-of-Experts）"><a href="#（3）MoE-Fusion：混合专家（Mixture-of-Experts）" class="headerlink" title="（3）MoE-Fusion：混合专家（Mixture of Experts）"></a>（3）MoE-Fusion：混合专家（Mixture of Experts）</h4><blockquote><p>Cao 等人提出了 <strong>MoE - Fusion</strong> 方法，集成了局部 - 全局专家混合模型来动态提取各自模态的有效特征。这种对局部信息和全局对比的动态特征学习证明了目标检测的有效性。</p></blockquote><ul><li>框架：<ul><li>设置多个“专家”：IR 专家、VIS 专家、局部专家、全局专家等；</li><li>门控网络依据输入内容动态分配专家权重；</li></ul></li><li>优点：<ul><li>自适应选择“此处更信 IR 还是 VIS”；</li><li>同一模型可适应多场景（昼夜、雾天、复杂背景）。</li></ul></li></ul><h4 id="（4）MetaFusion：元特征嵌入"><a href="#（4）MetaFusion：元特征嵌入" class="headerlink" title="（4）MetaFusion：元特征嵌入"></a>（4）MetaFusion：元特征嵌入</h4><blockquote><p>Zhao 等人提出了 <strong>MetaFusion</strong> 方法，利用来自目标检测的元特征嵌入来对齐语义和融合特征，实现有效的联合学习。</p></blockquote><ul><li>想法：<ul><li>从检测网络中抽取“元特征”（例如类别、尺度、置信度等）；</li><li>将这些 meta-feature 作为额外输入嵌入融合网络；</li></ul></li><li>作用：<ul><li>元特征提供了高层语义指导；</li><li>促使融合结果在语义上更“合检测器口味”。</li></ul></li></ul><h4 id="（5）不显式生成融合图的多模态检测方法（对照）"><a href="#（5）不显式生成融合图的多模态检测方法（对照）" class="headerlink" title="（5）不显式生成融合图的多模态检测方法（对照）"></a>（5）不显式生成融合图的多模态检测方法（对照）</h4><p>在此论文中也提到一些<strong>不生成融合图，只做特征级多模态检测</strong>的工作：</p><ul><li>通常结构：<ul><li>IR&#x2F;VIS 双流编码器；</li><li>中间若干层做特征级融合（注意力 &#x2F; 特征对齐 &#x2F; 概率集成等）；</li><li>最后单个检测头预测类别与框。</li></ul></li></ul><blockquote><p>这些方法从“检测任务”出发看问题，而本文综述更多是从“通用融合 + 下游任务”的角度出发。</p><p>换而言之，这些方法只关心结果，不生成融合图，直接做多模态检测，只要任务完成得好就行。</p></blockquote><hr><h3 id="3-2-2-面向语义分割的融合"><a href="#3-2-2-面向语义分割的融合" class="headerlink" title="3.2.2 面向语义分割的融合"></a>3.2.2 面向语义分割的融合</h3><p>目标：使融合图对语义分割任务友好，提高 mIoU&#x2F;mAcc。</p><h4 id="（1）SeAFusion：融合-分割级联-语义损失"><a href="#（1）SeAFusion：融合-分割级联-语义损失" class="headerlink" title="（1）SeAFusion：融合 + 分割级联 + 语义损失"></a>（1）SeAFusion：融合 + 分割级联 + 语义损失</h4><blockquote><p><strong>SeAFusion</strong> 将图像融合与分割任务级联，通过基于循环的训练引入语义损失以增强融合的信息丰富度。</p></blockquote><ul><li>结构：FusionNet → SegNet 级联；</li><li>训练：<ol><li>用分割网络的预测与 GT 计算分割损失（比如交叉熵 + mIoU）；</li><li>把这部分损失反向传到 FusionNet；</li></ol></li><li>作用：<ul><li>分割网络像“老师”一样指导融合网络：<ul><li>哪些边界要更清晰；</li><li>哪些区域需要更高对比度。</li></ul></li></ul></li></ul><h4 id="（2）SuperFusion：多功能配准-融合-语义感知框架"><a href="#（2）SuperFusion：多功能配准-融合-语义感知框架" class="headerlink" title="（2）SuperFusion：多功能配准 + 融合 + 语义感知框架"></a>（2）SuperFusion：多功能配准 + 融合 + 语义感知框架</h4><blockquote><p>Tang 等人提出了 <strong>SuperFusion</strong>，这是一个用于多模态图像配准、融合和语义感知的通用框架。</p></blockquote><ul><li>同时解决三个问题：<ol><li>未对齐图像的配准（估计双向形变场）；</li><li>IR&#x2F;VIS 信息融合；</li><li>语义感知（通过语义分支提供高层约束）。</li></ol></li><li>特点：<ul><li>在未对齐场景下表现突出；</li><li>是“数据兼容 + 融合 + 任务适配三合一”的代表工作。</li></ul></li></ul><h4 id="（3）PSFusion：渐进式语义注入"><a href="#（3）PSFusion：渐进式语义注入" class="headerlink" title="（3）PSFusion：渐进式语义注入"></a>（3）PSFusion：渐进式语义注入</h4><blockquote><p><strong>PSFusion</strong> 在特征级别引入渐进式语义注入，考虑了融合的语义需求。它还表明，在计算资源较少的情况下，图像级融合在感知任务中与特征融合具有可比的性能。</p></blockquote><ul><li>思路：<ul><li>在<strong>特征层</strong>（而不是图像层）注入语义特征；</li><li>分割网络多个层级的语义特征分别注入融合 backbone 的不同 stage；</li></ul></li><li>实验观察：<ul><li>在算力受限时，“图像级融合 + 单模态分割”在性能上可以接近“特征级多模态分割”；</li><li>对工程实现有参考价值（便于复用现成分割器）。</li></ul></li></ul><h4 id="（4）SegMiF：双任务相关性的联合学习"><a href="#（4）SegMiF：双任务相关性的联合学习" class="headerlink" title="（4）SegMiF：双任务相关性的联合学习"></a>（4）SegMiF：双任务相关性的联合学习</h4><blockquote><p>Liu 等人提出了 <strong>SegMiF</strong>，利用双任务相关性来提高分割和融合性能，引入分层交互注意力进行细粒度任务映射，并收集了这些任务的最大全时基准数据集。</p></blockquote><ul><li>特点：<ul><li>同时看“融合任务”和“分割任务”的相关性；</li><li>用分层交互注意力模块进行双向特征交换：<ul><li>融合分支获得语义信息；</li><li>分割分支利用更丰富的结构与纹理特征；</li></ul></li></ul></li><li>数据：<ul><li>作者构建了目前规模最大的融合 + 分割联合数据集 <strong>FMB</strong>（14 类），用于性能评测。</li></ul></li></ul><h4 id="（5）MRFS-CAF-TIMFusion-等统一感知驱动框架"><a href="#（5）MRFS-CAF-TIMFusion-等统一感知驱动框架" class="headerlink" title="（5）MRFS &#x2F; CAF &#x2F; TIMFusion 等统一感知驱动框架"></a>（5）MRFS &#x2F; CAF &#x2F; TIMFusion 等统一感知驱动框架</h4><ul><li><p><strong>MRFS</strong>：</p><blockquote><p>Zhang 等人提出了 <strong>MRFS</strong>，这是一个耦合学习框架，通过相互强化集成图像融合和分割，实现了增强的视觉质量和更准确的分割结果。</p></blockquote><ul><li>耦合学习框架，强调“融合结果 ↔ 分割结果”的互相强化；</li></ul></li><li><p><strong>CAF</strong>：</p><ul><li>自动搜索适合检测 &#x2F; 分割任务的损失组合；</li><li>减少人工调 loss 的工作量；</li></ul></li><li><p><strong>TIMFusion</strong>：</p><ul><li>用隐式架构搜索获得硬件友好、任务性能优良的融合网络；</li><li>一套结构适配多种下游感知任务。</li></ul></li></ul><hr><h3 id="3-2-3-其他感知任务（略）"><a href="#3-2-3-其他感知任务（略）" class="headerlink" title="3.2.3 其他感知任务（略）"></a>3.2.3 其他感知任务（略）</h3><p>这些任务与 IVIF 是“应用层面的延伸”，与任务本身不完全以 IVIF 为核心，因此简略描述：</p><ul><li><p><strong>RGB-T 目标跟踪</strong></p><ul><li>通过像素级 &#x2F; 特征级 &#x2F; 决策级融合 IR&#x2F;VIS 提高跟踪鲁棒性；</li><li>抵抗遮挡、弱光、背景干扰。</li></ul></li><li><p><strong>人群计数（Crowd Counting）</strong></p><ul><li>利用多模态信息区分“人”和“背景热源”；</li><li>在高密度、遮挡严重场景下提高密度估计精度。</li></ul></li><li><p><strong>显著目标检测（SOD）</strong></p><ul><li>如 IRFS：将融合和显著性检测做成多任务框架；</li><li>通过交互增强使显著目标轮廓更准确。</li></ul></li><li><p><strong>深度估计 &#x2F; 其他</strong></p><ul><li>使用光谱转换 &#x2F; 风格迁移处理复杂光照；</li><li>辅助估计深度或其他几何信息。</li></ul></li></ul><hr><h2 id="3-3-数据兼容性：未配准、通用融合与对抗攻击"><a href="#3-3-数据兼容性：未配准、通用融合与对抗攻击" class="headerlink" title="3.3 数据兼容性：未配准、通用融合与对抗攻击"></a>3.3 数据兼容性：未配准、通用融合与对抗攻击</h2><blockquote><p>理论上的红外和可见光图像的融合是美好的，但是现实是残酷的：未对齐、带噪声、甚至遭攻击。</p></blockquote><h3 id="3-3-1-未配准（Misaligned）图像的融合"><a href="#3-3-1-未配准（Misaligned）图像的融合" class="headerlink" title="3.3.1 未配准（Misaligned）图像的融合"></a>3.3.1 未配准（Misaligned）图像的融合</h3><p><img data-src="/./../images/image-20251130093438617.png" alt="image-20251130093438617"></p><p>大致分为两条路线：</p><ol><li>基于风格迁移的“伪标签”方法（Style Transfer-based）</li><li>基于潜在空间的模态无关特征（Latent Space-based）</li></ol><h4 id="（1）基于风格迁移的配准-融合范式"><a href="#（1）基于风格迁移的配准-融合范式" class="headerlink" title="（1）基于风格迁移的配准 + 融合范式"></a>（1）基于风格迁移的配准 + 融合范式</h4><p>生成伪标签，将多模态配准转换为单模态配准</p><p>典型流程：</p><ol><li><strong>MTN（模态转换网络）</strong>：<ul><li>将 VIS → IR 风格 或 IR → VIS 风格，生成“伪标签图像”；</li></ul></li><li><strong>STN（空间变换网络）</strong>：<ul><li>学习源图像与伪标签之间的空间变形场；</li></ul></li><li>完成配准后，再用常规融合网络处理。</li></ol><p>代表方法：</p><ul><li><p><strong>Nemar &#x2F; UMIR</strong></p><ul><li>Nemar：提出使用单模态度量指标训练多模态配准网络；</li><li>双向训练策略：<ul><li>先配准再转换；</li><li>先转换再配准；</li></ul></li><li>目的是保证伪标签既风格接近目标模态，又几何结构可靠。</li></ul></li><li><p><strong>UMFusion</strong></p><ul><li>关注“未配准导致的重影伪影”；</li><li>引入跨模态生成-配准范式：<ul><li>生成伪标签减少模态差异；</li><li>联合学习配准与融合，缓解重影。</li></ul></li></ul></li><li><p><strong>RFNet</strong></p><ul><li>把配准与融合设计成互相促进的两个子任务：<ul><li>粗配准 → 融合 → 精配准 → 再融合；</li></ul></li><li>在配准过程中不断利用融合结果提升一致性。</li></ul></li></ul><p>优点：</p><ul><li>利用单模态指标做监督，训练相对稳定；</li><li>宜于处理较大视角差异。</li></ul><p>缺点：</p><ul><li>模态转换质量会直接影响最终效果；</li><li>有可能引入风格偏差（比如颜色失真）。</li></ul><h4 id="（2）基于潜在空间的模态无关特征"><a href="#（2）基于潜在空间的模态无关特征" class="headerlink" title="（2）基于潜在空间的模态无关特征"></a>（2）基于潜在空间的模态无关特征</h4><p>构建模态无关的特征空间，将多模态图像特征映射到共享空间，并利用共享特征预测变形场</p><p>核心思想：</p><ul><li>提取 IR &#x2F; VIS 的“公共特征”，让两种模态在“共享空间”对齐；</li><li>在共享特征空间中预测形变场，再反向约束原图配准。</li></ul><p>代表方法：</p><ul><li><p><strong>MURF</strong></p><ul><li>强调模态无关特征对配准的重要性；</li><li>采用粗配准 + 精配准两阶段策略：<ul><li>粗配准：对比学习约束共享特征的分布；</li><li>精配准：在共享空间上细化形变参数；</li></ul></li><li>融合与配准互相扶持。</li></ul></li><li><p><strong>SuperFusion</strong></p><ul><li>前面已经介绍：融合 + 配准 + 语义一体化；</li><li>在光度一致性和端点误差约束下估计双向形变场；</li><li>对几何扭曲修正效果好。</li></ul></li><li><p><strong>ReCoNet</strong></p><ul><li>递归修正网络；</li><li>多次迭代补偿几何扭曲，逐步减轻重影。</li></ul></li><li><p><strong>SemLA &#x2F; RFVIF</strong></p><ul><li>SemLA：在各阶段显式嵌入语义信息，以辅助配准；</li><li>RFVIF：针对融合质量做特别优化。</li></ul></li></ul><p><strong>定量结论（基于 RoadScene &#x2F; M³FD）</strong>：</p><ul><li>有真实变形场监督的 <strong>SuperFusion</strong> 在基于参考的指标（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>I</mi></mrow><annotation encoding="application/x-tex">MI</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mord mathnormal" style="margin-right:.07847em">I</span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mi>I</mi><mi>F</mi></mrow><annotation encoding="application/x-tex">VIF</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span><span class="mord mathnormal" style="margin-right:.07847em">I</span><span class="mord mathnormal" style="margin-right:.13889em">F</span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>C</mi></mrow><annotation encoding="application/x-tex">CC</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.07153em">CC</span></span></span></span>、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>A</mi></msub><mi>B</mi><mi mathvariant="normal">/</mi><mi>F</mi></mrow><annotation encoding="application/x-tex">Q_AB/F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:.05017em">B</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:.13889em">F</span></span></span></span>）最强；</li><li>风格迁移系列（UMFusion &#x2F; IMF）基于参考指标次优；</li><li>潜在空间系列（RFVIF &#x2F; SemLA &#x2F; ReCoNet &#x2F; MURF）在无参考指标（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">EN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.10903em">EN</span></span></span></span> &#x2F; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>F</mi></mrow><annotation encoding="application/x-tex">SF</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.13889em">SF</span></span></span></span> &#x2F; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>D</mi></mrow><annotation encoding="application/x-tex">SD</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="mord mathnormal" style="margin-right:.02778em">D</span></span></span></span> &#x2F; <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>G</mi></mrow><annotation encoding="application/x-tex">AG</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.6833em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">G</span></span></span></span>）上通常更好 → 纹理丰富。</li></ul><blockquote><p>说明：配准质量、融合质量、纹理丰富度、语义表现等之间存在复杂 trade-off。</p></blockquote><hr><h3 id="3-3-2-通用融合框架（General-Fusion）"><a href="#3-3-2-通用融合框架（General-Fusion）" class="headerlink" title="3.3.2 通用融合框架（General Fusion）"></a>3.3.2 通用融合框架（General Fusion）</h3><p>目标：一套框架尽可能适配多种模态、多种任务：</p><ul><li>IR&#x2F;VIS；</li><li>医学多模态（CT&#x2F;MRI&#x2F;PET 等）；</li><li>遥感多光谱 &#x2F; 全色等。</li></ul><p>为了解决多模态图像融合中通用融合框架的核心问题：</p><ul><li><p>损失函数增强：创新集中在先进的损失度量上，以提高图像质量。</p></li><li><p>架构创新：升级旨在更好地进行特征提取和提高网络效率。</p></li></ul><h4 id="（1）损失函数创新方向"><a href="#（1）损失函数创新方向" class="headerlink" title="（1）损失函数创新方向"></a>（1）损失函数创新方向</h4><ul><li><p><strong>U2Fusion &#x2F; FusionDN &#x2F; PMGI &#x2F; IFCNN &#x2F; SDNet 等</strong></p><ul><li>从信息论、图像质量评价（IQA）、边缘&#x2F;梯度等角度设计通用损失；</li><li>使得同一套损失对多种任务（IVIF、医学融合、曝光融合…）都适用。</li></ul></li><li><p><strong>CoCoNet</strong></p><ul><li>提出特征级对比学习损失；</li><li>通过拉近“好融合样本”与“源图像 &#x2F; 参考”的距离，拉远“差融合样本”；</li><li>在无参考指标（EN &#x2F; SF &#x2F; AG）上表现非常突出。</li></ul></li><li><p><strong>EMMA</strong></p><ul><li>引入伪感知模块（pseudo-perceptual module），通过自监督学习模拟人类感知；</li><li>再配合感知损失约束融合结果。</li></ul></li><li><p><strong>FILM</strong></p><ul><li>利用 LLM 生成语义提示，并通过跨模态注意力引导融合；</li><li>把“文本语义”引入通用融合框架中。</li></ul></li></ul><h4 id="（2）架构设计方向"><a href="#（2）架构设计方向" class="headerlink" title="（2）架构设计方向"></a>（2）架构设计方向</h4><ul><li>多任务共享编码器 + 任务特定解码器；</li><li>隐式架构搜索（IAS）：<ul><li>TIMFusion 等通过搜索得到既兼顾视觉（指标）又兼顾硬件（延迟）的结构；</li></ul></li><li>多层次分解（基础层 &#x2F; 细节层）+ 双层优化（BIMDL 的思路）。</li></ul><hr><h3 id="3-3-3-对抗攻击与鲁棒性"><a href="#3-3-3-对抗攻击与鲁棒性" class="headerlink" title="3.3.3 对抗攻击与鲁棒性"></a>3.3.3 对抗攻击与鲁棒性</h3><p>对抗攻击是指在图像上添加难以察觉的扰动，很容易欺骗神经网络的估计。网络在多模态视觉下对抗攻击的脆弱性尚未得到广泛研究。考虑到多模态分割在对抗攻击下的鲁棒性，PAIFusion 率先利用图像融合来增强鲁棒性。该工作通过详细分析确定了脆弱的融合操作和规则。</p><p>意义：对确保现实世界应用的鲁棒性和安全性至关重要。</p><p><strong>问题背景</strong></p><ul><li>多模态视觉网络也会被对抗攻击欺骗；</li><li>如果 IR 或 VIS 某一模态被添加微小扰动，融合结果和下游任务很可能出现严重错误；</li><li>IVIF 如何在这种情况下保持鲁棒性，是刚刚起步的方向。</li></ul><p><img data-src="/./../images/image-20251130100736225.png" alt="image-20251130100736225"></p><blockquote><p>图7. 数据兼容的红外与可见光图像融合（IVIF）方法的基本分阶段流程。</p></blockquote><p><strong>PAIFusion</strong></p><ul><li>首个系统分析“融合操作 &#x2F; 规则在对抗攻击下的脆弱性”的工作；</li><li>利用图像融合增强多模态分割网络在对抗攻击下的鲁棒性；</li><li>分析了哪些融合模块容易被攻击、哪些更稳定。</li></ul><blockquote><p>未来研究：<br>设计“对抗鲁棒融合网络”，即使 IR&#x2F;VIS 中有一模态被攻击，融合结果仍可用。</p></blockquote><hr><h2 id="3-4-架构总结与讨论"><a href="#3-4-架构总结与讨论" class="headerlink" title="3.4 架构总结与讨论"></a>3.4 架构总结与讨论</h2><p>论文把现有深度融合网络按结构复杂度和设计思路分为四类：</p><ol><li><p><strong>使用现有架构</strong></p><ul><li>直接使用 ResNet &#x2F; DenseNet &#x2F; U-Net 等成熟架构；</li><li>典型：DenseFuse、RFN-Nest 等；</li><li>优点：开发成本低、训练稳定。</li></ul></li><li><p><strong>复杂堆叠网络</strong></p><ul><li>把多种模块（残差块、注意力、Transformer、金字塔、GNN 等）堆在一起；</li><li>典型：Re2Fusion、CDDFuse、TGFuse 等；</li><li>强调极致性能，但结构复杂，难以部署。</li></ul></li><li><p><strong>多分支 &#x2F; 多批次架构</strong></p><ul><li>为不同模态（IR&#x2F;VIS）设计不同编码器；</li><li>常见于任务适配网络（TarDAL &#x2F; DetFusion &#x2F; MoE-Fusion 等）。</li></ul></li><li><p><strong>递归 &#x2F; 扩散结构</strong></p><ul><li>使用递归网络或扩散模型逐步优化融合结果；</li><li>典型：ReCoNet、DDFM；</li><li>适合质量要求极高或序列数据，但代价是计算量巨大。</li></ul></li></ol><p>总体而言，红外和可见光图像融合的研究正朝着更复杂和精细的网络结构发展，以满足复杂的融合需求。</p><hr><h2 id="3-5-损失函数总结与讨论"><a href="#3-5-损失函数总结与讨论" class="headerlink" title="3.5 损失函数总结与讨论"></a>3.5 损失函数总结与讨论</h2><p>在无监督红外和可见光图像融合领域，损失函数的设计和选择至关重要。</p><p>这些函数通常可从像素级别、评估指标和数据特征三个主要维度进行理解和分类。（本节不讨论 GAN 和扩散模型等生成模型）</p><p>在像素级别，L1 和 MSE 损失函数通过直接比较像素来评估图像相似性。SSIM 作为关键的评估指标，通过考虑图像结构和质量扩展了这一概念，反映了人类视觉感知。针对数据特征的损失函数（如图像梯度）则侧重于在融合过程中保留详细纹理。</p><p>在此基础上，出现了更复杂的损失函数变体来解决图像融合中的特定挑战。例如，在像素级别应用视觉显著性图（VSM）代表了一种创新，产生了更细致的融合效果。在评估指标层面，使用空间频率（SF）等不太常见的指标作为损失函数，强调了图像的频率特征和视觉效果，从而在保持视觉舒适度的同时实现有效融合。此外，基于复杂图像特征的损失函数（如最大化梯度或边缘提取，以及包括感知和对比损失）提供了更深入的见解和解决方案。</p><p>损失函数的设计多种多样，除了上述类型，还引入了许多专门设计的损失函数。研究人员可以根据源图像的特征和任务要求选择、组合和优化这些损失函数，推动图像融合领域的进一步发展和创新。</p><p><strong>三大维度</strong>：</p><ol><li><p><strong>像素级损失</strong></p><ul><li>L1 &#x2F; MSE：直接度量像素差；</li><li><strong>SSIM</strong>：考虑结构 &#x2F; 亮度 &#x2F; 对比度，更符合人眼感知。</li></ul></li><li><p><strong>结构 &#x2F; 特征级损失</strong></p><ul><li>梯度 &#x2F; 边缘损失：保持纹理和边缘清晰；</li><li>频域损失（如 SF）：强调细节和高频信息；</li><li>视觉显著性图（VSM）：对显著区域赋予更大权重；</li><li>NR-IQA（无参考图像质量评价）指标也被用作损失（FusionDN 等）。</li></ul></li><li><p><strong>语义 &#x2F; 对比 &#x2F; 感知损失</strong></p><ul><li>语义损失：利用分类 &#x2F; 检测 &#x2F; 分割网络的中间特征；</li><li>对比损失：CoCoNet、EMMA 等充分利用；</li><li>感知损失：基于 VGG 等预训练网络的高维特征差异；</li><li>任务驱动损失：检测 &#x2F; 分割的任务损失直接反向训练融合网络（TarDAL、SeAFusion、SegMiF 等）。</li></ul></li></ol><blockquote><p>实际做法：<br>通常是多种损失混合加权，通过验证集（包括感知指标）来调整权重。</p></blockquote><hr><h1 id="4-基准数据集与评估指标"><a href="#4-基准数据集与评估指标" class="headerlink" title="4. 基准数据集与评估指标"></a>4. 基准数据集与评估指标</h1><h2 id="4-1-数据集概览"><a href="#4-1-数据集概览" class="headerlink" title="4.1 数据集概览"></a>4.1 数据集概览</h2><p><img data-src="/./../images/image-20251130101340887.png" alt="image-20251130101340887"></p><blockquote><p>图8. 现有红外-可见光图像融合（IVIF）数据集示意图</p></blockquote><p><img data-src="/./../images/image-20251130101421522.png" alt="image-20251130101421522"></p><p>大致分三类：</p><h3 id="4-1-1-早期面向融合的数据集"><a href="#4-1-1-早期面向融合的数据集" class="headerlink" title="4.1.1 早期面向融合的数据集"></a>4.1.1 早期面向融合的数据集</h3><ul><li><p><strong>TNO</strong></p><ul><li>军事 &#x2F; 户外场景；</li><li>图像对数较少（261 对），分辨率较低（768×576）；</li><li>常用作视觉效果展示与定性评估。</li></ul></li><li><p><strong>RoadScene</strong></p><ul><li>道路驾驶场景，包含不同天气；</li><li>图像数量 221 对，分辨率不固定；</li><li>更接近自动驾驶场景。</li></ul></li><li><p><strong>VIFB</strong></p><ul><li>整理多个来源数据；</li><li>更偏向“benchmark 性质”，用于对比传统方法。</li></ul></li></ul><h3 id="4-1-2-目标检测数据集"><a href="#4-1-2-目标检测数据集" class="headerlink" title="4.1.2 目标检测数据集"></a>4.1.2 目标检测数据集</h3><ul><li><p><strong>MS</strong></p><ul><li>多光谱目标检测数据集；</li><li>主要是驾驶场景，包含 6 类目标；</li><li>有标注框，可用于检测任务。</li></ul></li><li><p><strong>LLVIP</strong></p><ul><li>监控场景，1280×720 分辨率；</li><li>主要关注行人检测；</li><li>适合做低光 &#x2F; 夜间监控场景研究。</li></ul></li><li><p><strong>M³FD</strong></p><ul><li>当前最重要的 IVIF 检测数据集；</li><li>4200 对图像，1024×768 分辨率，多角度拍摄；</li><li>涵盖白天 &#x2F; 夜晚 &#x2F; 雾 &#x2F; 雨等多种天气；</li><li>标注 6 类目标，含 3 万+ 标注框；</li><li>大量任务驱动方法（TarDAL &#x2F; DetFusion 等）都在上面评测。</li></ul></li></ul><h3 id="4-1-3-语义分割数据集"><a href="#4-1-3-语义分割数据集" class="headerlink" title="4.1.3 语义分割数据集"></a>4.1.3 语义分割数据集</h3><ul><li><p><strong>MFNet</strong></p><ul><li>1569 对图像，640×480 分辨率；</li><li>驾驶场景，8 类语义标签；</li><li>适用于 RGB-T 语义分割研究。</li></ul></li><li><p><strong>FMB</strong></p><ul><li>1500 对图像，800×600 分辨率；</li><li>多角度，14 类语义标签；</li><li>由 SegMiF 提出，专门用于“融合 + 分割”的联合评估。</li></ul></li></ul><hr><h2 id="4-2-评估指标"><a href="#4-2-评估指标" class="headerlink" title="4.2 评估指标"></a>4.2 评估指标</h2><h3 id="4-2-1-融合质量指标（9-个）"><a href="#4-2-1-融合质量指标（9-个）" class="headerlink" title="4.2.1 融合质量指标（9 个）"></a>4.2.1 融合质量指标（9 个）</h3><p><strong>有参考（基于源图像）</strong>：</p><ol><li><p><strong>MI（Mutual Information，互信息）</strong></p><p>衡量源图像信息被传递到融合图的程度（信息量）；</p></li><li><p><strong>VIF（Visual Information Fidelity）</strong></p><p>评估融合的保真度，与人类视觉系统对齐，值越高表示性能越好；</p></li><li><p><strong>CC（Correlation Coefficient）</strong></p><p>评估融合图像对源图像的反映程度，侧重于线性相关性；</p></li><li><p><strong>SCD（Sum of Correlation of Differences）</strong></p><p>衡量源图像独特信息的整合情况。CC 强调现有关系，而 SCD 关注新元素；</p></li><li><p><strong><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Q</mi><mi>A</mi></msub><mi>B</mi><mi mathvariant="normal">/</mi><mi>F</mi></mrow><annotation encoding="application/x-tex">Q_AB/F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3283em"><span style="top:-2.55em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:.05017em">B</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:.13889em">F</span></span></span></span></span>（基于梯度的融合质量指标）</strong></p><p>看边缘 &#x2F; 梯度细节是否得到良好保留（越接近 1 越好）。</p></li></ol><p><strong>无参考指标</strong>：</p><ol start="6"><li><strong>EN（Entropy，熵）</strong>：衡量融合图像中的信息内容（信息量大小）（但对噪声敏感）；</li><li><strong>SF（Spatial Frequency，空间频率）</strong>：评估细节和纹理的清晰度，值越高表示边缘和纹理信息越丰富；</li><li><strong>SD（Standard Deviation，标准差）</strong>：从分布和对比度方面反映图像质量，对比度越高，SD 值越大；</li><li><strong>AG（Average Gradient，平均梯度）</strong>：衡量纹理特征和细节，AG 值越高表示融合性能越好。</li></ol><blockquote><p>注意：EN &#x2F; SF &#x2F; AG 等指标高不一定就好，有可能是噪声多。</p></blockquote><hr><h3 id="4-2-2-配准指标"><a href="#4-2-2-配准指标" class="headerlink" title="4.2.2 配准指标"></a>4.2.2 配准指标</h3><ol><li><p><strong>MSE（Mean Squared Error）</strong></p><p>测量两幅图像像素之间的平均平方差，用于评估它们的对齐程度。MSE 值越低，表示相似度越高，是图像配准和融合中的关键指标；</p></li><li><p><strong>MI（互信息）</strong></p><p>是图像配准中常用的相似性指标。MI 值越高，表明两幅图像对齐越好；</p></li><li><p><strong>NCC（Normalized Cross Correlation）</strong></p><p>是评估两幅图像中对应窗口相似性的指标，用于评价配准精度。</p></li></ol><hr><h3 id="4-2-3-感知任务指标"><a href="#4-2-3-感知任务指标" class="headerlink" title="4.2.3 感知任务指标"></a>4.2.3 感知任务指标</h3><ul><li><p><strong>语义分割</strong></p><ul><li>IoU &#x2F; mIoU：交并比及其类别平均；</li><li>Acc &#x2F; mAcc：像素准确率及其类别平均。</li></ul></li><li><p><strong>目标检测</strong></p><ul><li>Precision &#x2F; Recall：精确率 &#x2F; 召回率；</li><li>AP &#x2F; mAP：单类 &#x2F; 多类平均精度。</li></ul></li></ul><blockquote><p>评估时最好把“融合指标 + 感知指标”一起看，单纯依赖 MI &#x2F; VIF 容易误判。</p></blockquote><hr><h1 id="5-性能总结与消融对比"><a href="#5-性能总结与消融对比" class="headerlink" title="5. 性能总结与消融对比"></a>5. 性能总结与消融对比</h1><h2 id="5-1-图像融合质量（TNO-RoadScene-M³FD）"><a href="#5-1-图像融合质量（TNO-RoadScene-M³FD）" class="headerlink" title="5.1 图像融合质量（TNO &#x2F; RoadScene &#x2F; M³FD）"></a>5.1 图像融合质量（TNO &#x2F; RoadScene &#x2F; M³FD）</h2><p><strong>定性分析</strong></p><p><img data-src="/./../images/image-20251130101959966.png" alt="image-20251130101959966"></p><ul><li>在 M³FD 的烟雾场景中对比多种方法：<ul><li>IGNet：为了压制烟雾，倾向 IR 模态，结果颜色失真严重；</li><li>CoCoNet：人物和背景车都清楚，但草地区域发生明显颜色偏差；</li><li>CAF &#x2F; MoEFusion：在压制烟雾的同时，保持颜色自然。</li></ul></li></ul><p><strong>定量分析</strong></p><p><img data-src="/./../images/image-20251130102235155.png" alt="image-20251130102235155"></p><ul><li><p>参考指标：</p><ul><li>使用传统基于源图像损失函数的方法（如 DenseFuse 的 CC 指标）由于充分保留了源图像信息而具有显著优势；</li><li>针对分割进行优化的方法在视觉保真度指标（如 PromptF 的 VIF 指标）上也表现出优势；</li><li>原因：它们充分利用了语义信息。</li></ul></li><li><p>无参考指标：</p><ul><li>CoCoNet 利用特征级对比损失优化整个训练过程，其表示能力远超传统损失函数，从而在性能上脱颖而出。</li><li>说明对比学习在无 GT 场景中的优势。</li></ul></li></ul><hr><h2 id="5-2-未对齐图像融合性能（RoadScene-M³FD）"><a href="#5-2-未对齐图像融合性能（RoadScene-M³FD）" class="headerlink" title="5.2 未对齐图像融合性能（RoadScene &#x2F; M³FD）"></a>5.2 未对齐图像融合性能（RoadScene &#x2F; M³FD）</h2><p><strong>定性结果</strong></p><p><img data-src="/./../images/image-20251130102417905.png" alt="image-20251130102417905"></p><blockquote><p>图10. 未对齐的多模态图像在两个典型图像对上的表现，与多种最新的图像融合方法进行了对比。</p></blockquote><ul><li>SuperFusion &#x2F; UMFusion &#x2F; IMF 等提供了比较好的配准与融合效果：<ul><li>有效消除结构失真与边缘重影；</li></ul></li><li>潜在空间系列（RFVIF &#x2F; SemLA &#x2F; ReCoNet &#x2F; MURF）仍有少量残留变形，但纹理更丰富。</li></ul><p><strong>定量结果</strong></p><p><img data-src="/./../images/image-20251130102532854.png" alt="image-20251130102532854"></p><ul><li>SuperFusion 凭借真实变形场监督，在 MI &#x2F; VIF &#x2F; CC &#x2F; Q_AB&#x2F;F 上表现最佳；</li><li>UMFusion &#x2F; IMF 在大多数参考指标上次优；</li><li>MURF 等在无参考指标（EN &#x2F; SF &#x2F; SD &#x2F; AG）表现更好。</li></ul><blockquote><p>结论：风格迁移 + 潜在空间方法各有优劣；<br>若能结合两者优势，可能得到更通用的未配准融合框架。</p></blockquote><hr><h2 id="5-3-面向目标检测的融合性能（M³FD-YOLOv5）"><a href="#5-3-面向目标检测的融合性能（M³FD-YOLOv5）" class="headerlink" title="5.3 面向目标检测的融合性能（M³FD + YOLOv5）"></a>5.3 面向目标检测的融合性能（M³FD + YOLOv5）</h2><p><strong>定性对比</strong></p><p><img data-src="/./../images/image-20251130102609933.png" alt="image-20251130102609933"></p><ul><li>小目标场景（桥上行人 + 雾 &#x2F; 强光）：<ul><li>DDFM &#x2F; LRRNet &#x2F; ReCoNet：对比度低，小目标不突出；</li><li>DDcGAN &#x2F; IRFS：伪影明显，引入错误检测；</li><li>TarDAL &#x2F; PAIFusion：行人全部被检出，小目标召回高。</li></ul></li></ul><p><strong>定量结果</strong></p><p><img data-src="/./../images/image-20251130102629383.png" alt="image-20251130102629383"></p><ul><li>TarDAL 在 mAP 上最高，特别是 People、Car、Bus 等重点类别；</li><li>TIMFusion &#x2F; SegMiF &#x2F; SDNet 等感知驱动方法也有不错表现；</li><li>一些视觉上很优的 GAN &#x2F; 扩散方法，在检测任务上并不占优势。</li></ul><blockquote><p>进一步说明：“<strong>任务驱动训练的融合方法 &gt; 只看图像指标的融合方法</strong>”。</p></blockquote><hr><h2 id="5-4-面向语义分割的融合性能（FMB-SegFormer）"><a href="#5-4-面向语义分割的融合性能（FMB-SegFormer）" class="headerlink" title="5.4 面向语义分割的融合性能（FMB + SegFormer）"></a>5.4 面向语义分割的融合性能（FMB + SegFormer）</h2><p>统一使用先进的 SegFormer 作为基础分割器，在一致的设置下对各种先进融合方法进行训练，以衡量它们的性能。采用 FMB 数据集的官方训练集和测试集划分。</p><p><strong>定性比较</strong>：</p><p><img data-src="/./../images/image-20251130103612720.png" alt="image-20251130103612720"></p><blockquote><p>图12. 基于图像融合的语义分割结果与几种最先进的方法进行了比较</p></blockquote><p>在这个场景中，由于能见度低，位于黑暗区域的行人对大多数方法来说都难以准确分割。同时，公交车发出的强光导致场景过曝，使得许多方法将公交车误分类为汽车，且无法捕捉其完整轮廓。这些挑战凸显了处理极端光照条件场景的持续困难。</p><p><strong>定量比较</strong>：在表 6 中，我们报告了语义分割任务的数值结果。</p><p><img data-src="/./../images/image-20251130103701946.png" alt="image-20251130103701946"></p><ul><li><p>实验设置：</p><ul><li>采用 SegFormer 作为统一分割器；</li><li>各融合方法先生成融合图，再用融合图训练 SegFormer；</li><li>使用 FMB 官方划分训练 &#x2F; 测试。</li></ul></li><li><p>观察结论：</p><ul><li>MoEFusion &#x2F; DetFusion &#x2F; SegMiF &#x2F; MRFS 等任务驱动方法在 mIoU &#x2F; mAcc 上表现最优；</li><li>单纯视觉驱动的融合方法在极端光照（黑夜 &#x2F; 强光）场景的分割边界上有明显误差；</li><li>再次验证：<strong>语义信息（分割 &#x2F; 检测）对融合网络设计有直接帮助。</strong></li></ul></li></ul><hr><h2 id="5-5-计算效率与资源消耗"><a href="#5-5-计算效率与资源消耗" class="headerlink" title="5.5 计算效率与资源消耗"></a>5.5 计算效率与资源消耗</h2><p><img data-src="/./../images/image-20251130103948338.png" alt="image-20251130103948338"></p><p>论文统计了在 M³FD 上处理 10 张 1024×768 图像的三项指标：</p><ul><li>运行时间（ms）</li><li>参数量（M）</li><li>FLOPs（G）</li></ul><p><strong>总体趋势：</strong></p><ul><li>DenseFuse &#x2F; U2Fusion 等早期方法结构简单，速度最快，参数最少，但性能略落后；</li><li>Transformer &#x2F; 扩散模型（如 DDFM）计算开销巨大，运行时间可达数十万 ms，不适合实时应用；</li><li>TIMFusion 借助隐式架构搜索，在速度与性能间做了一个比较好的平衡；</li><li>任务驱动方法通常比纯视觉优化方法稍重，但收益体现在感知任务上。</li></ul><blockquote><p>实际应用（自动驾驶 &#x2F; 边缘设备）中，需要注意<br>“mAP &#x2F; mIoU 与 FPS &#x2F; 耗能之间的 trade-off”。</p></blockquote><hr><h1 id="6-挑战与未来研究方向"><a href="#6-挑战与未来研究方向" class="headerlink" title="6. 挑战与未来研究方向"></a>6. 挑战与未来研究方向</h1><p>论文最后总结了若干重要开放问题，这里挑与 IVIF 强相关的部分：</p><h2 id="6-1-未配准-对抗鲁棒性"><a href="#6-1-未配准-对抗鲁棒性" class="headerlink" title="6.1 未配准 &amp; 对抗鲁棒性"></a>6.1 未配准 &amp; 对抗鲁棒性</h2><ul><li>未对齐问题：<ul><li>当前方法只能应对“中等程度”的视角偏差；</li><li>大视角变化、非刚性形变、动态场景仍待解决。</li></ul></li><li>对抗攻击：<ul><li>多模态网络在对抗扰动下表现不明朗；</li><li>如何“利用多模态互补性提升鲁棒性”是值得深入挖掘的问题。</li></ul></li></ul><h2 id="6-2-任务驱动的融合设计"><a href="#6-2-任务驱动的融合设计" class="headerlink" title="6.2 任务驱动的融合设计"></a>6.2 任务驱动的融合设计</h2><ul><li>传统：只优化融合指标；</li><li>未来：以 Detection &#x2F; Segmentation &#x2F; Tracking &#x2F; SOD 等任务的表现为第一目标；</li><li>关键挑战：<ul><li>怎样设计 loss 与网络结构，使视觉质量和任务性能不过分冲突；</li><li>怎样在多任务（检测+分割+跟踪）联合训练时避免互相干扰。</li></ul></li></ul><h2 id="6-3-更大、更真实、更难的数据集"><a href="#6-3-更大、更真实、更难的数据集" class="headerlink" title="6.3 更大、更真实、更难的数据集"></a>6.3 更大、更真实、更难的数据集</h2><ul><li>现有数据集在场景多样性、极端天气、复杂动态环境方面仍然有限；</li><li>期待：<ul><li>城市 &#x2F; 高速 &#x2F; 隧道 &#x2F; 森林 &#x2F; 灾害现场等多场景大规模多模态数据；</li><li>带有检测 &#x2F; 分割 &#x2F; 深度 &#x2F; SOD 等多任务标注的统一数据集。</li></ul></li></ul><h2 id="6-4-更合理的评价体系"><a href="#6-4-更合理的评价体系" class="headerlink" title="6.4 更合理的评价体系"></a>6.4 更合理的评价体系</h2><ul><li>单一的 EN &#x2F; SF &#x2F; AG 等指标无法全面反映质量；</li><li>单看 mAP &#x2F; mIoU 也不能体现“观感差异”；</li><li>未来可能需要：<ul><li>结合 IQA &#x2F; VQA &#x2F; 感知指标的综合评分体系；</li><li>甚至“人 + 模型”的混合评价框架。</li></ul></li></ul><h2 id="6-5-轻量化与部署友好"><a href="#6-5-轻量化与部署友好" class="headerlink" title="6.5 轻量化与部署友好"></a>6.5 轻量化与部署友好</h2><ul><li>Transformer &#x2F; 扩散 &#x2F; GAN 型网络部署到无人机、车载、智能相机时常常超出算力预算；</li><li>可能方向：<ul><li>网络剪枝、蒸馏、权值共享；</li><li>硬件感知的 NAS &#x2F; IAS；</li><li>面向 IR&#x2F;VIS 特性的专用算子设计。</li></ul></li></ul></div><footer class="post-footer"><div class="post-copyright"><ul><li class="post-copyright-author"><strong>本文作者： </strong>MaJianyu</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://mjy.js.org/2025/11/29/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%20%20TPAMI%202024%20%20IVIF%E4%BB%BB%E5%8A%A1%E8%BF%9B%E5%B1%95%E5%92%8C%E5%BA%94%E7%94%A8%E6%A6%82%E8%BF%B0%EF%BC%9A%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%85%BC%E5%AE%B9%E6%80%A7%E5%88%B0%E4%BB%BB%E5%8A%A1%E9%80%82%E9%85%8D%E6%80%A7%E3%80%91/" title="【论文阅读 | TPAMI | IVIF任务进展和应用概述：从数据兼容性到任务适配性】">https://mjy.js.org/2025/11/29/【论文阅读 TPAMI 2024 IVIF任务进展和应用概述：从数据兼容性到任务适配性】/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="post-tags"><a href="/tags/%E5%A4%9A%E5%85%89%E8%B0%B1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag"># 多光谱目标检测</a> <a href="/tags/IVIF%E4%BB%BB%E5%8A%A1/" rel="tag"># IVIF任务</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2025/11/11/2025.11.11%20C%E8%AF%AD%E8%A8%80%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8A%E6%9C%BA%E5%AE%9E%E4%B9%A0%E5%9B%9B/" rel="prev" title="2025.11.11 C语言程序设计上机实习四"><i class="fa fa-angle-left"></i> 2025.11.11 C语言程序设计上机实习四</a></div><div class="post-nav-item"><a href="/2025/12/02/2025.12.02%20C%E8%AF%AD%E8%A8%80%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8A%E6%9C%BA%E5%AE%9E%E4%B9%A0%E4%BA%94/" rel="next" title="2025.12.02 C语言程序设计上机实习五">2025.12.02 C语言程序设计上机实习五 <i class="fa fa-angle-right"></i></a></div></div></footer></article></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2021 – <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">MaJianyu</span></div><div class="wordcount"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-chart-line"></i> </span><span>站点总字数：</span> <span title="站点总字数">199k</span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span>站点阅读时长 &asymp;</span> <span title="站点阅读时长">6:02</span></span></div><div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div><a href="https://icp.gov.moe/?keyword=20216556" target="_blank">萌ICP备20216556号</a></div></footer><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><div class="sidebar-dimmer"></div><div class="back-to-top" role="button" aria-label="返回顶部"><i class="fa fa-arrow-up fa-lg"></i> <span>0%</span></div><a href="https://github.com/majianyu2007" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><script src="https://mjy.js.org/js/darkmode@1.5.7.min.js"></script><script>var options = {
  bottom: '16px',
  right: '16px',
  left: 'unset',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();</script></body></html>